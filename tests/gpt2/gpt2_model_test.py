import pytest
import torch
import torch.nn as nn

from llms_implementation.gpt2.model import (
    GELU,
    LayerNorm,
    FeedForwardNetwork,
    TransformerBlock,
    GPT2,
)


def _mean_std(t):
    return float(t.mean().cpu()), float(t.std().cpu())


# ---------- Construction & shapes ----------
@pytest.mark.parametrize(
    "cfg",
    [
        {
            "embed_dim": 64,
            "seq_len": 16,
            "num_head": 4,
            "num_layers": 3,
            "vocab_size": 100,
        },
        {
            "embed_dim": 128,
            "seq_len": 32,
            "num_head": 8,
            "num_layers": 1,
            "vocab_size": 257,
        },
    ],
)
def test_gpt2_builds_and_shapes(cfg):
    m = GPT2(cfg)

    # layers count and types
    assert isinstance(m.layers, nn.ModuleList)
    assert len(m.layers) == cfg["num_layers"]
    assert all(isinstance(b, TransformerBlock) for b in m.layers)

    # embeddings (note: model.py defines wte as (vocab_size, seq_len) in THIS repo)
    assert isinstance(m.wte, nn.Embedding)
    assert m.wte.weight.shape == (cfg["vocab_size"], cfg["seq_len"])

    assert isinstance(m.wpe, nn.Embedding)
    assert m.wpe.weight.shape == (cfg["seq_len"], cfg["embed_dim"])

    # final heads / norms
    assert isinstance(m.ln_f, LayerNorm)
    assert isinstance(m.lm_head, nn.Linear)
    assert m.lm_head.weight.shape == (cfg["vocab_size"], cfg["embed_dim"])


# ---------- Attn config propagation ----------
def test_gpt2_attn_config_propagates():
    cfg = {
        "embed_dim": 48,
        "seq_len": 12,
        "num_head": 6,
        "num_layers": 2,
        "vocab_size": 50,
        "attn_config": {"dropout_p": 0.0, "use_fused": True},
    }
    m = GPT2(cfg)
    for blk in m.layers:
        attn = blk.causal_attn
        # We don't assume specific class internals beyond these attributes existing
        assert hasattr(attn, "dropout")
        # p should be 0.0 as provided
        p = getattr(attn.dropout, "p", None)
        assert p == 0.0


# ---------- State dict sanity / parameter counts ----------
def test_gpt2_param_counts_monotonic_with_layers():
    base = {"embed_dim": 64, "seq_len": 16, "num_head": 4, "vocab_size": 101}
    m1 = GPT2({**base, "num_layers": 1})
    m2 = GPT2({**base, "num_layers": 4})

    n1 = sum(p.numel() for p in m1.parameters())
    n2 = sum(p.numel() for p in m2.parameters())
    assert n2 > n1  # more layers â†’ more params


def _roll_forward_no_tokens(model: GPT2, h: torch.Tensor) -> torch.Tensor:
    """
    Since GPT2.forward(token_ids) isn't defined in this repo, we manually
    roll a hidden state through the stack: layers -> ln_f -> lm_head.
    """
    for blk in model.layers:
        h = blk(h)
    h = model.ln_f(h)
    logits = model.lm_head(h)
    return logits


# ---------- construction, wiring, and shapes (no assumptions about init) ----------
@pytest.mark.parametrize(
    "cfg",
    [
        {
            "embed_dim": 64,
            "seq_len": 16,
            "num_head": 4,
            "num_layers": 2,
            "vocab_size": 101,
        },
        {
            "embed_dim": 128,
            "seq_len": 32,
            "num_head": 8,
            "num_layers": 1,
            "vocab_size": 257,
        },
    ],
)
def test_gpt2_wiring_and_output_shapes(cfg):
    torch.manual_seed(0)
    m = GPT2(cfg)

    # sanity on module types
    assert isinstance(m.layers, nn.ModuleList)
    assert len(m.layers) == cfg["num_layers"]
    assert all(isinstance(b, TransformerBlock) for b in m.layers)
    assert isinstance(m.ln_f, LayerNorm)
    assert isinstance(m.lm_head, nn.Linear)

    # roll a random hidden state through the model
    B, T, E, V = 2, cfg["seq_len"], cfg["embed_dim"], cfg["vocab_size"]
    h = torch.randn(B, T, E)
    logits = _roll_forward_no_tokens(m, h)

    assert logits.shape == (B, T, V)
    assert torch.isfinite(logits).all()


# ---------- gradient flow end-to-end through blocks/ln_f/lm_head ----------
def test_gpt2_backward_through_stack():
    cfg = {
        "embed_dim": 96,
        "seq_len": 24,
        "num_head": 6,
        "num_layers": 3,
        "vocab_size": 321,
    }
    torch.manual_seed(0)
    m = GPT2(cfg)

    B, T, E = 3, cfg["seq_len"], cfg["embed_dim"]
    h = torch.randn(B, T, E, requires_grad=True)

    logits = _roll_forward_no_tokens(m, h)
    loss = (logits**2).mean()
    loss.backward()

    # gradients reach inputs and at least some params
    assert h.grad is not None and torch.isfinite(h.grad).all()
    assert any(
        (p.grad is not None) and torch.isfinite(p.grad).all() for p in m.parameters()
    )


# ---------- dropout behavior: train vs eval should differ when attn_pdrop > 0 ----------
@pytest.mark.parametrize("pdrop", [0.3])
def test_gpt2_train_eval_diverge_when_dropout_on(pdrop):
    cfg = {
        "embed_dim": 64,
        "seq_len": 16,
        "num_head": 4,
        "num_layers": 2,
        "vocab_size": 111,
        # don't rely on config key names; we'll set .p directly below
        "attn_config": {"use_fused": True},
    }
    torch.manual_seed(0)
    m = GPT2(cfg)

    # Force attention dropout p on all blocks (supports either .dropout or .attn_drop)
    for blk in m.layers:
        attn = blk.causal_attn
        drop = getattr(attn, "dropout", None) or getattr(attn, "attn_drop", None)
        assert drop is not None, "Attention module lacks a dropout submodule"
        drop.p = pdrop

    B, T, E = 2, cfg["seq_len"], cfg["embed_dim"]
    h = torch.randn(B, T, E)

    m.train()
    y_train = _roll_forward_no_tokens(m, h)

    m.eval()
    y_eval = _roll_forward_no_tokens(m, h)

    # With dropout>0, train vs eval must differ
    assert not torch.allclose(y_train, y_eval)


# ---------- determinism when dropout is off ----------
def test_gpt2_determinism_when_dropout_off():
    cfg = {
        "embed_dim": 64,
        "seq_len": 16,
        "num_head": 4,
        "num_layers": 2,
        "vocab_size": 111,
        "attn_config": {"attn_pdrop": 0.0, "use_fused": True},
    }
    torch.manual_seed(123)
    m = GPT2(cfg).eval()
    B, T, E = 2, cfg["seq_len"], cfg["embed_dim"]
    h = torch.randn(B, T, E)

    y1 = _roll_forward_no_tokens(m, h)
    y2 = _roll_forward_no_tokens(m, h)

    torch.testing.assert_close(y1, y2, rtol=0, atol=0)


# ---------- device portability (CUDA if available) ----------
@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
def test_gpt2_cuda_parity_cpu():
    cfg = {
        "embed_dim": 64,
        "seq_len": 12,
        "num_head": 4,
        "num_layers": 2,
        "vocab_size": 99,
        "attn_config": {"attn_pdrop": 0.0, "use_fused": True},
    }
    torch.manual_seed(0)
    m_cpu = GPT2(cfg).eval()
    m_gpu = GPT2(cfg).eval().cuda()
    # copy weights for parity
    m_gpu.load_state_dict(m_cpu.state_dict())

    B, T, E = 2, cfg["seq_len"], cfg["embed_dim"]
    h = torch.randn(B, T, E)
    y_cpu = _roll_forward_no_tokens(m_cpu, h)
    y_gpu = _roll_forward_no_tokens(m_gpu, h.cuda()).cpu()

    torch.testing.assert_close(y_cpu, y_gpu, rtol=1e-4, atol=1e-4)


# ---------- parameter count monotonicity with num_layers ----------
def test_gpt2_param_counts_increase_with_depth():
    base = {"embed_dim": 64, "seq_len": 16, "num_head": 4, "vocab_size": 101}
    m1 = GPT2({**base, "num_layers": 1})
    m4 = GPT2({**base, "num_layers": 4})
    n1 = sum(p.numel() for p in m1.parameters())
    n4 = sum(p.numel() for p in m4.parameters())
    assert n4 > n1


# ---------- state_dict roundtrip ----------
def test_gpt2_state_dict_roundtrip():
    cfg = {
        "embed_dim": 48,
        "seq_len": 12,
        "num_head": 6,
        "num_layers": 2,
        "vocab_size": 50,
    }
    torch.manual_seed(0)
    m = GPT2(cfg)
    sd = m.state_dict()
    m2 = GPT2(cfg)
    missing, unexpected = m2.load_state_dict(sd, strict=True)
    assert missing == [] and unexpected == []
    # sanity that outputs match after load
    B, T, E = 2, cfg["seq_len"], cfg["embed_dim"]
    h = torch.randn(B, T, E)
    y1 = _roll_forward_no_tokens(m.eval(), h)
    y2 = _roll_forward_no_tokens(m2.eval(), h)
    torch.testing.assert_close(y1, y2, rtol=0, atol=0)


# ---------- seq_len guard: T must not exceed configured context ----------
def test_block_raises_if_seq_len_exceeded():
    cfg = {
        "embed_dim": 32,
        "seq_len": 8,
        "num_head": 4,
        "num_layers": 1,
        "vocab_size": 77,
    }
    m = GPT2(cfg)
    blk = m.layers[0]

    B, T_over, E = 1, cfg["seq_len"] + 1, cfg["embed_dim"]
    h = torch.randn(B, T_over, E)

    attn = blk.causal_attn
    # If the attention enforces a fixed causal mask length (manual path),
    # we expect an assertion. Otherwise, just verify it runs and shapes match.
    expect_assert = False
    if getattr(attn, "use_fused", False) is False and hasattr(attn, "_get_causal_mask"):
        # typical manual-path implementations with a prebuilt mask assert on overflow
        expect_assert = True

    if expect_assert:
        with pytest.raises(AssertionError):
            _ = blk(h)
    else:
        out = blk(h)
        assert out.shape == (B, T_over, E)


# ---------- ln_f affects output (parameter sensitivity sanity) ----------
def test_ln_f_parameter_effect():
    cfg = {
        "embed_dim": 40,
        "seq_len": 10,
        "num_head": 5,
        "num_layers": 2,
        "vocab_size": 80,
    }
    m = GPT2(cfg).eval()
    B, T, E = 2, cfg["seq_len"], cfg["embed_dim"]
    h = torch.randn(B, T, E)

    y1 = _roll_forward_no_tokens(m, h)

    # Nudge ln_f.gamma and ensure outputs change
    with torch.no_grad():
        m.ln_f.gamma.mul_(1.1)

    y2 = _roll_forward_no_tokens(m, h)
    assert not torch.allclose(y1, y2)
